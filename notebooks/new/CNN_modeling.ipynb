{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Split\n",
    "Split dataset into train (70%), test (20%) and val (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "Train set: 700 files (70 per class)\n",
      "Validation set: 100 files (10 per class)\n",
      "Test set: 200 files (20 per class)\n",
      "\n",
      "Class distribution (in order):\n",
      "blues:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "classical:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "country:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "disco:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "hiphop:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "jazz:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "metal:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "pop:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "reggae:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "rock:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_splits(root_folder, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, files_per_class=100):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test splits for audio files while maintaining class balance\n",
    "    and keeping files ordered by class.\n",
    "    \n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder containing class subfolders\n",
    "        train_ratio (float): Ratio of files to use for training (default: 0.7)\n",
    "        val_ratio (float): Ratio of files to use for validation (default: 0.1)\n",
    "        test_ratio (float): Ratio of files to use for testing (default: 0.2)\n",
    "        files_per_class (int): Number of files in each class (default: 100)\n",
    "    \"\"\"\n",
    "    # Verify ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Calculate number of files for each split\n",
    "    n_train = int(files_per_class * train_ratio)  # 70 files\n",
    "    n_val = int(files_per_class * val_ratio)      # 10 files\n",
    "    n_test = files_per_class - n_train - n_val    # 20 files\n",
    "    \n",
    "    # Get sorted list of subfolders (classes)\n",
    "    class_folders = sorted([d for d in os.listdir(root_folder) \n",
    "                          if os.path.isdir(os.path.join(root_folder, d))])\n",
    "    \n",
    "    # Initialize lists for each split\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    \n",
    "    # Process each class in order\n",
    "    for class_name in class_folders:\n",
    "        class_path = os.path.join(root_folder, class_name)\n",
    "        \n",
    "        # Get sorted list of audio files\n",
    "        audio_files = sorted([f for f in os.listdir(class_path)\n",
    "                            if f.lower().endswith(('.wav', '.mp3', '.flac', '.m4a', '.ogg'))])\n",
    "        \n",
    "        assert len(audio_files) == files_per_class, f\"Expected {files_per_class} files in {class_name}, found {len(audio_files)}\"\n",
    "        \n",
    "        # Create relative paths\n",
    "        class_files = [os.path.join(class_name, f) for f in audio_files]\n",
    "        \n",
    "        # Shuffle files while maintaining reproducibility\n",
    "        random.seed(hash(class_name))  # Use class name as seed for consistent shuffling\n",
    "        random.shuffle(class_files)\n",
    "        \n",
    "        # Split files\n",
    "        train_files.extend(class_files[:n_train])\n",
    "        val_files.extend(class_files[n_train:n_train + n_val])\n",
    "        test_files.extend(class_files[n_train + n_val:])\n",
    "    \n",
    "    # Write splits to files, maintaining class order\n",
    "    def write_split(file_name, files):\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            for file_path in files:\n",
    "                f.write(f\"{file_path}\\n\")\n",
    "    \n",
    "    write_split('../../datasets/train.txt', train_files)\n",
    "    write_split('../../datasets/val.txt', val_files)\n",
    "    write_split('../../datasets/test.txt', test_files)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Dataset split complete:\")\n",
    "    print(f\"Train set: {len(train_files)} files ({n_train} per class)\")\n",
    "    print(f\"Validation set: {len(val_files)} files ({n_val} per class)\")\n",
    "    print(f\"Test set: {len(test_files)} files ({n_test} per class)\")\n",
    "    \n",
    "    # Print class distribution in order\n",
    "    print(\"\\nClass distribution (in order):\")\n",
    "    for class_name in class_folders:\n",
    "        train_count = sum(1 for f in train_files if class_name in f)\n",
    "        val_count = sum(1 for f in val_files if class_name in f)\n",
    "        test_count = sum(1 for f in test_files if class_name in f)\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  Train: {train_count}\")\n",
    "        print(f\"  Val: {val_count}\")\n",
    "        print(f\"  Test: {test_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Replace with your root folder path\n",
    "    dataset_path = \"../../datasets/genres_original\"\n",
    "    \n",
    "    create_dataset_splits(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "# import soundfile as sf\n",
    "import librosa as lb\n",
    "from torch.utils import data\n",
    "from audiomentations import (\n",
    "    Compose,\n",
    "    AddGaussianNoise,\n",
    "    TimeStretch,\n",
    "    PitchShift,\n",
    "    Shift\n",
    ")\n",
    "# from torchaudio_augmentations import (\n",
    "#     RandomResizedCrop,\n",
    "#     RandomApply,\n",
    "#     PolarityInversion,\n",
    "#     Noise,\n",
    "#     Gain,\n",
    "#     HighLowPass,\n",
    "#     Delay,\n",
    "#     PitchShift,\n",
    "#     Reverb,\n",
    "#     Compose,\n",
    "# )\n",
    "\n",
    "\n",
    "GTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "\n",
    "class GTZANDataset(data.Dataset):\n",
    "    def __init__(self, data_path, split):\n",
    "        self.data_path =  data_path if data_path else ''\n",
    "        self.split = split\n",
    "        # self.is_augmentation = is_augmentation\n",
    "        self.genres = GTZAN_GENRES\n",
    "        self._get_song_list()\n",
    "        # if is_augmentation:\n",
    "        #     self._get_augmentations()\n",
    "\n",
    "    def _get_song_list(self):\n",
    "        list_filename = os.path.join(self.data_path, '%s.txt' % self.split)\n",
    "        with open(list_filename) as f:\n",
    "            lines = f.readlines()\n",
    "        self.song_list = [line.strip() for line in lines]\n",
    "\n",
    "    def _get_augmentations(self):\n",
    "        augment = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "            PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "            Shift(p=0.5)\n",
    "        ])\n",
    "        self.augmentation = augment(self.samples, self.sample_rate)\n",
    "\n",
    "    # def _adjust_audio_length(self, wav):\n",
    "    #     if self.split == 'train':\n",
    "    #         random_index = random.randint(0, len(wav) - self.num_samples - 1)\n",
    "    #         wav = wav[random_index : random_index + self.num_samples]\n",
    "    #     else:\n",
    "    #         hop = (len(wav) - self.num_samples) // self.num_chunks\n",
    "    #         wav = np.array([wav[i * hop : i * hop + self.num_samples] for i in range(self.num_chunks)])\n",
    "    #     return wav\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.song_list[index]\n",
    "\n",
    "        # get genre\n",
    "        genre_name = line.split('/')[0]\n",
    "        genre_index = self.genres.index(genre_name)\n",
    "\n",
    "        # get audio\n",
    "        audio_filename = os.path.join(self.data_path, 'genres', line)\n",
    "        self.samples, self.sample_rate = lb.load(audio_filename)\n",
    "\n",
    "\n",
    "        # # adjust audio length\n",
    "        # wav = self._adjust_audio_length(wav).astype('float32')\n",
    "\n",
    "        # data augmentation\n",
    "        # if self.is_augmentation:\n",
    "        #     samples = self.augmentation(torch.from_numpy(self.samples).unsqueeze(0)).squeeze(0).numpy()\n",
    "\n",
    "        return genre_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_list)\n",
    "\n",
    "def get_dataloader(data_path=None, \n",
    "                   split='train', \n",
    "                   batch_size=16, \n",
    "                   num_workers=0\n",
    "                   ):\n",
    "    is_shuffle = True if (split == 'train') else False\n",
    "    data_loader = data.DataLoader(dataset=GTZANDataset(data_path, \n",
    "                                                       split \n",
    "                                                       ),\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=is_shuffle,\n",
    "                                  drop_last=False,\n",
    "                                  num_workers=num_workers)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'hiphop\\\\hiphop.00007.wav' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\notebooks\\new\\CNN_modeling.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loader \u001b[39m=\u001b[39m get_dataloader(data_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../../datasets/\u001b[39m\u001b[39m'\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m iter_train_loader \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_wav, train_genre \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iter_train_loader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m valid_loader \u001b[39m=\u001b[39m get_dataloader(split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test_loader \u001b[39m=\u001b[39m get_dataloader(split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\notebooks\\new\\CNN_modeling.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# get genre\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m genre_name \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m genre_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenres\u001b[39m.\u001b[39;49mindex(genre_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# get audio\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m audio_filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_path, \u001b[39m'\u001b[39m\u001b[39mgenres\u001b[39m\u001b[39m'\u001b[39m, line)\n",
      "\u001b[1;31mValueError\u001b[0m: 'hiphop\\\\hiphop.00007.wav' is not in list"
     ]
    }
   ],
   "source": [
    "train_loader = get_dataloader(data_path='../../datasets/', split='train')\n",
    "iter_train_loader = iter(train_loader)\n",
    "train_wav, train_genre = next(iter_train_loader)\n",
    "\n",
    "valid_loader = get_dataloader(split='valid')\n",
    "test_loader = get_dataloader(split='test')\n",
    "iter_test_loader = iter(test_loader)\n",
    "test_wav, test_genre = next(iter_test_loader)\n",
    "print('training data shape: %s' % str(train_wav.shape))\n",
    "print('validation/test data shape: %s' % str(test_wav.shape))\n",
    "print(train_genre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
