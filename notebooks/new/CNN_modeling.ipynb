{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Split\n",
    "Split dataset into train (70%), test (20%) and val (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "Train set: 700 files (70 per class)\n",
      "Validation set: 100 files (10 per class)\n",
      "Test set: 200 files (20 per class)\n",
      "\n",
      "Class distribution (in order):\n",
      "blues:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "classical:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "country:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "disco:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "hiphop:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "jazz:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "metal:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "pop:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "reggae:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "rock:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_splits(root_folder, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, files_per_class=100):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test splits for audio files while maintaining class balance\n",
    "    and keeping files ordered by class.\n",
    "    \n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder containing class subfolders\n",
    "        train_ratio (float): Ratio of files to use for training (default: 0.7)\n",
    "        val_ratio (float): Ratio of files to use for validation (default: 0.1)\n",
    "        test_ratio (float): Ratio of files to use for testing (default: 0.2)\n",
    "        files_per_class (int): Number of files in each class (default: 100)\n",
    "    \"\"\"\n",
    "    # Verify ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Calculate number of files for each split\n",
    "    n_train = int(files_per_class * train_ratio)  # 70 files\n",
    "    n_val = int(files_per_class * val_ratio)      # 10 files\n",
    "    n_test = files_per_class - n_train - n_val    # 20 files\n",
    "    \n",
    "    # Get sorted list of subfolders (classes)\n",
    "    class_folders = sorted([d for d in os.listdir(root_folder) \n",
    "                          if os.path.isdir(os.path.join(root_folder, d))])\n",
    "    \n",
    "    # Initialize lists for each split\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    \n",
    "    # Process each class in order\n",
    "    for class_name in class_folders:\n",
    "        class_path = os.path.join(root_folder, class_name)\n",
    "        \n",
    "        # Get sorted list of audio files\n",
    "        audio_files = sorted([f for f in os.listdir(class_path)\n",
    "                            if f.lower().endswith(('.wav', '.mp3', '.flac', '.m4a', '.ogg'))])\n",
    "        \n",
    "        assert len(audio_files) == files_per_class, f\"Expected {files_per_class} files in {class_name}, found {len(audio_files)}\"\n",
    "        \n",
    "        # Create relative paths\n",
    "        class_files = [os.path.join(class_name, f) for f in audio_files]\n",
    "        \n",
    "        # Shuffle files while maintaining reproducibility\n",
    "        random.seed(hash(class_name))  # Use class name as seed for consistent shuffling\n",
    "        random.shuffle(class_files)\n",
    "        \n",
    "        # Split files\n",
    "        train_files.extend(class_files[:n_train])\n",
    "        val_files.extend(class_files[n_train:n_train + n_val])\n",
    "        test_files.extend(class_files[n_train + n_val:])\n",
    "    \n",
    "    # Write splits to files, maintaining class order\n",
    "    def write_split(file_name, files):\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            for file_path in files:\n",
    "                f.write(f\"{file_path}\\n\")\n",
    "    \n",
    "    write_split('../../datasets/train.txt', train_files)\n",
    "    write_split('../../datasets/val.txt', val_files)\n",
    "    write_split('../../datasets/test.txt', test_files)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Dataset split complete:\")\n",
    "    print(f\"Train set: {len(train_files)} files ({n_train} per class)\")\n",
    "    print(f\"Validation set: {len(val_files)} files ({n_val} per class)\")\n",
    "    print(f\"Test set: {len(test_files)} files ({n_test} per class)\")\n",
    "    \n",
    "    # Print class distribution in order\n",
    "    print(\"\\nClass distribution (in order):\")\n",
    "    for class_name in class_folders:\n",
    "        train_count = sum(1 for f in train_files if class_name in f)\n",
    "        val_count = sum(1 for f in val_files if class_name in f)\n",
    "        test_count = sum(1 for f in test_files if class_name in f)\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  Train: {train_count}\")\n",
    "        print(f\"  Val: {val_count}\")\n",
    "        print(f\"  Test: {test_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Replace with your root folder path\n",
    "    dataset_path = \"../../datasets/genres_original\"\n",
    "    \n",
    "    create_dataset_splits(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import random\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "import librosa as lb\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from audiomentations import (\n",
    "    Compose,\n",
    "    AddGaussianNoise,\n",
    "    TimeStretch,\n",
    "    PitchShift,\n",
    "    Shift\n",
    ")\n",
    "# from torchaudio_augmentations import (\n",
    "#     RandomResizedCrop,\n",
    "#     RandomApply,\n",
    "#     PolarityInversion,\n",
    "#     Noise,\n",
    "#     Gain,\n",
    "#     HighLowPass,\n",
    "#     Delay,\n",
    "#     PitchShift,\n",
    "#     Reverb,\n",
    "#     Compose,\n",
    "# )\n",
    "\n",
    "\n",
    "GTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "\n",
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, data_path, split):\n",
    "        self.data_path =  data_path if data_path else ''\n",
    "        self.split = split\n",
    "        # self.is_augmentation = is_augmentation\n",
    "        self.genres = GTZAN_GENRES\n",
    "        self._get_song_list()\n",
    "        # if is_augmentation:\n",
    "        #     self._get_augmentations()\n",
    "\n",
    "    def _get_song_list(self):\n",
    "        list_filename = os.path.join(self.data_path, '%s.txt' % self.split)\n",
    "        with open(list_filename) as f:\n",
    "            lines = f.readlines()\n",
    "        self.song_list = [line.strip() for line in lines]\n",
    "\n",
    "    def _get_augmentations(self):\n",
    "        augment = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "            PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "            Shift(p=0.5)\n",
    "        ])\n",
    "        self.augmentation = augment(self.samples, self.sample_rate)\n",
    "\n",
    "    # def _adjust_audio_length(self, wav):\n",
    "    #     if self.split == 'train':\n",
    "    #         random_index = random.randint(0, len(wav) - self.num_samples - 1)\n",
    "    #         wav = wav[random_index : random_index + self.num_samples]\n",
    "    #     else:\n",
    "    #         hop = (len(wav) - self.num_samples) // self.num_chunks\n",
    "    #         wav = np.array([wav[i * hop : i * hop + self.num_samples] for i in range(self.num_chunks)])\n",
    "    #     return wav\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.song_list[index]\n",
    "\n",
    "        # get genre\n",
    "        genre_name = line.split('\\\\')[0]\n",
    "        genre_index = self.genres.index(genre_name)\n",
    "\n",
    "        # get audio\n",
    "        audio_filename = os.path.join(self.data_path, \"genres_original\", line)\n",
    "        self.samples, self.sample_rate = lb.load(audio_filename)\n",
    "\n",
    "\n",
    "        # # adjust audio length\n",
    "        # wav = self._adjust_audio_length(wav).astype('float32')\n",
    "\n",
    "        # data augmentation\n",
    "        # if self.is_augmentation:\n",
    "        #     samples = self.augmentation(torch.from_numpy(self.samples).unsqueeze(0)).squeeze(0).numpy()\n",
    "\n",
    "        return genre_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_list)\n",
    "\n",
    "def get_dataloader(data_path=None, \n",
    "                   split='train', \n",
    "                   batch_size=16, \n",
    "                   num_workers=0\n",
    "                   ):\n",
    "    is_shuffle = True if (split == 'train') else False\n",
    "    data_loader = DataLoader(dataset=GTZANDataset(data_path, \n",
    "                                                       split \n",
    "                                                       ),\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=is_shuffle,\n",
    "                                  drop_last=False,\n",
    "                                  num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001B62BFF5F90>\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: torch.Size([16])\n",
      "validation/test data shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Directory\n",
    "directory=\"../../datasets/\"\n",
    "\n",
    "# Load train data\n",
    "train_loader = get_dataloader(data_path=directory, split='train')\n",
    "print(train_loader)\n",
    "print(type(train_loader))\n",
    "iter_train_loader = iter(train_loader)\n",
    "train_data = next(iter_train_loader)\n",
    "\n",
    "# Load validation data\n",
    "valid_loader = get_dataloader(data_path=directory, split='val')\n",
    "\n",
    "# # Load test data\n",
    "test_loader = get_dataloader(data_path=directory, split='test')\n",
    "\n",
    "iter_test_loader = iter(test_loader)\n",
    "test_data = next(iter_test_loader)\n",
    "print('training data shape: %s' % str(train_data.shape))\n",
    "print('validation/test data shape: %s' % str(test_data.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Conv_2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, shape=3, pooling=2, dropout=0.1):\n",
    "        super(Conv_2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, shape, padding=shape//2)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(pooling)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, wav):\n",
    "        out = self.conv(wav)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import numpy as np\n",
    "# import librosa\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_channels=16, \n",
    "                       sample_rate=22050, \n",
    "                       n_fft=1024, \n",
    "                       f_min=0.0, \n",
    "                       f_max=11025.0, \n",
    "                       num_mels=128, \n",
    "                       num_classes=10):\n",
    "        # hop_length=512\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # mel spectrogram\n",
    "        self.melspec = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, \n",
    "                                                            n_fft=n_fft, \n",
    "                                                            f_min=f_min, \n",
    "                                                            f_max=f_max, \n",
    "                                                            n_mels=num_mels)\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        # Librosa alternative: Not feasible due to necessity of audio for function declaration\n",
    "        # D = np.abs(librosa.stft(y, n_fft = n_fft, hop_length = hop_length))\n",
    "        # # Convert an amplitude spectrogram to Decibels-scaled spectrogram.\n",
    "        # DB = librosa.amplitude_to_db(D, ref = np.max)\n",
    "        self.input_bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        # convolutional layers\n",
    "        self.layer1 = Conv_2d(1, num_channels, pooling=(2, 3))\n",
    "        self.layer2 = Conv_2d(num_channels, num_channels, pooling=(3, 4))\n",
    "        self.layer3 = Conv_2d(num_channels, num_channels * 2, pooling=(2, 5))\n",
    "        self.layer4 = Conv_2d(num_channels * 2, num_channels * 2, pooling=(3, 3))\n",
    "        self.layer5 = Conv_2d(num_channels * 2, num_channels * 4, pooling=(3, 4))\n",
    "\n",
    "        # dense layers\n",
    "        self.dense1 = nn.Linear(num_channels * 4, num_channels * 4)\n",
    "        self.dense_bn = nn.BatchNorm1d(num_channels * 4)\n",
    "        self.dense2 = nn.Linear(num_channels * 4, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, wav):\n",
    "        # input Preprocessing\n",
    "        out = self.melspec(wav)\n",
    "        out = self.amplitude_to_db(out)\n",
    "\n",
    "        # input batch normalization\n",
    "        out = out.unsqueeze(1)\n",
    "        out = self.input_bn(out)\n",
    "\n",
    "        # convolutional layers\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        \n",
    "        # reshape. (batch_size, num_channels, 1, 1) -> (batch_size, num_channels)\n",
    "        out = out.reshape(len(out), -1)\n",
    "\n",
    "        # dense layers\n",
    "        out = self.dense1(out)\n",
    "        out = self.dense_bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\notebooks\\new\\CNN_modeling.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m cnn\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m (wav, genre_index) \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     wav \u001b[39m=\u001b[39m wav\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     genre_index \u001b[39m=\u001b[39m genre_index\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cnn = CNN().to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "valid_losses = []\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    # Train\n",
    "    cnn.train()\n",
    "    # for (wav, genre_index) in train_loader:\n",
    "    for value in train_loader:\n",
    "        print(value)\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        # Forward\n",
    "        out = cnn(wav)\n",
    "        loss = loss_function(out, genre_index)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print('Epoch: [%d/%d], Train loss: %.4f' % (epoch+1, num_epochs, np.mean(losses)))\n",
    "\n",
    "    # Validation\n",
    "    cnn.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    losses = []\n",
    "    for wav, genre_index in valid_loader:\n",
    "        wav = wav.to(device)\n",
    "        genre_index = genre_index.to(device)\n",
    "\n",
    "        # reshape and aggregate chunk-level predictions\n",
    "        b, c, t = wav.size()\n",
    "        logits = cnn(wav.view(-1, t))\n",
    "        logits = logits.view(b, c, -1).mean(dim=1)\n",
    "        loss = loss_function(logits, genre_index)\n",
    "        losses.append(loss.item())\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "        # append labels and predictions\n",
    "        y_true.extend(genre_index.tolist())\n",
    "        y_pred.extend(pred.tolist())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    valid_loss = np.mean(losses)\n",
    "    print('Epoch: [%d/%d], Valid loss: %.4f, Valid accuracy: %.4f' % (epoch+1, num_epochs, valid_loss, accuracy))\n",
    "\n",
    "    # Save model\n",
    "    valid_losses.append(valid_loss.item())\n",
    "    if np.argmin(valid_losses) == epoch:\n",
    "        print('Saving the best model at %d epochs!' % epoch)\n",
    "        torch.save(cnn.state_dict(), '../../models/best_model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
