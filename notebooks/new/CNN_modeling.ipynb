{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "Train set: 700 files (70 per class)\n",
      "Validation set: 100 files (10 per class)\n",
      "Test set: 200 files (20 per class)\n",
      "\n",
      "Class distribution (in order):\n",
      "blues:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "classical:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "country:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "disco:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "hiphop:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "jazz:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "metal:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "pop:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "reggae:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n",
      "rock:\n",
      "  Train: 70\n",
      "  Val: 10\n",
      "  Test: 20\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_splits(root_folder, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, files_per_class=100):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test splits for audio files while maintaining class balance\n",
    "    and keeping files ordered by class.\n",
    "    \n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder containing class subfolders\n",
    "        train_ratio (float): Ratio of files to use for training (default: 0.7)\n",
    "        val_ratio (float): Ratio of files to use for validation (default: 0.1)\n",
    "        test_ratio (float): Ratio of files to use for testing (default: 0.2)\n",
    "        files_per_class (int): Number of files in each class (default: 100)\n",
    "    \"\"\"\n",
    "    # Verify ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Calculate number of files for each split\n",
    "    n_train = int(files_per_class * train_ratio)  # 70 files\n",
    "    n_val = int(files_per_class * val_ratio)      # 10 files\n",
    "    n_test = files_per_class - n_train - n_val    # 20 files\n",
    "    \n",
    "    # Get sorted list of subfolders (classes)\n",
    "    class_folders = sorted([d for d in os.listdir(root_folder) \n",
    "                          if os.path.isdir(os.path.join(root_folder, d))])\n",
    "    \n",
    "    # Initialize lists for each split\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    \n",
    "    # Process each class in order\n",
    "    for class_name in class_folders:\n",
    "        class_path = os.path.join(root_folder, class_name)\n",
    "        \n",
    "        # Get sorted list of audio files\n",
    "        audio_files = sorted([f for f in os.listdir(class_path)\n",
    "                            if f.lower().endswith(('.wav', '.mp3', '.flac', '.m4a', '.ogg'))])\n",
    "        \n",
    "        assert len(audio_files) == files_per_class, f\"Expected {files_per_class} files in {class_name}, found {len(audio_files)}\"\n",
    "        \n",
    "        # Create relative paths\n",
    "        class_files = [os.path.join(class_name, f) for f in audio_files]\n",
    "        \n",
    "        # Shuffle files while maintaining reproducibility\n",
    "        random.seed(hash(class_name))  # Use class name as seed for consistent shuffling\n",
    "        random.shuffle(class_files)\n",
    "        \n",
    "        # Split files\n",
    "        train_files.extend(class_files[:n_train])\n",
    "        val_files.extend(class_files[n_train:n_train + n_val])\n",
    "        test_files.extend(class_files[n_train + n_val:])\n",
    "    \n",
    "    # Write splits to files, maintaining class order\n",
    "    def write_split(file_name, files):\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            for file_path in files:\n",
    "                f.write(f\"{file_path}\\n\")\n",
    "    \n",
    "    write_split('../../datasets/train.txt', train_files)\n",
    "    write_split('../../datasets/val.txt', val_files)\n",
    "    write_split('../../datasets/test.txt', test_files)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Dataset split complete:\")\n",
    "    print(f\"Train set: {len(train_files)} files ({n_train} per class)\")\n",
    "    print(f\"Validation set: {len(val_files)} files ({n_val} per class)\")\n",
    "    print(f\"Test set: {len(test_files)} files ({n_test} per class)\")\n",
    "    \n",
    "    # Print class distribution in order\n",
    "    print(\"\\nClass distribution (in order):\")\n",
    "    for class_name in class_folders:\n",
    "        train_count = sum(1 for f in train_files if class_name in f)\n",
    "        val_count = sum(1 for f in val_files if class_name in f)\n",
    "        test_count = sum(1 for f in test_files if class_name in f)\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  Train: {train_count}\")\n",
    "        print(f\"  Val: {val_count}\")\n",
    "        print(f\"  Test: {test_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Replace with your root folder path\n",
    "    dataset_path = \"../../datasets/genres_original\"\n",
    "    \n",
    "    create_dataset_splits(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "# import soundfile as sf\n",
    "import librosa as lb\n",
    "from torch.utils import data\n",
    "from audiomentations import (\n",
    "    Compose,\n",
    "    AddGaussianNoise,\n",
    "    TimeStretch,\n",
    "    PitchShift,\n",
    "    Shift\n",
    ")\n",
    "# from torchaudio_augmentations import (\n",
    "#     RandomResizedCrop,\n",
    "#     RandomApply,\n",
    "#     PolarityInversion,\n",
    "#     Noise,\n",
    "#     Gain,\n",
    "#     HighLowPass,\n",
    "#     Delay,\n",
    "#     PitchShift,\n",
    "#     Reverb,\n",
    "#     Compose,\n",
    "# )\n",
    "\n",
    "\n",
    "GTZAN_GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "\n",
    "class GTZANDataset(data.Dataset):\n",
    "    def __init__(self, data_path, split, is_augmentation):\n",
    "        self.data_path =  data_path if data_path else ''\n",
    "        self.split = split\n",
    "        self.is_augmentation = is_augmentation\n",
    "        self.genres = GTZAN_GENRES\n",
    "        self._get_song_list()\n",
    "        if is_augmentation:\n",
    "            self._get_augmentations()\n",
    "\n",
    "    def _get_song_list(self):\n",
    "        list_filename = os.path.join(self.data_path, '%s.txt' % self.split)\n",
    "        with open(list_filename) as f:\n",
    "            lines = f.readlines()\n",
    "        self.song_list = [line.strip() for line in lines]\n",
    "\n",
    "    def _get_augmentations(self):\n",
    "        augment = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "            PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "            Shift(p=0.5)\n",
    "        ])\n",
    "        self.augmentation = augment(self.samples, self.sample_rate)\n",
    "\n",
    "    # def _adjust_audio_length(self, wav):\n",
    "    #     if self.split == 'train':\n",
    "    #         random_index = random.randint(0, len(wav) - self.num_samples - 1)\n",
    "    #         wav = wav[random_index : random_index + self.num_samples]\n",
    "    #     else:\n",
    "    #         hop = (len(wav) - self.num_samples) // self.num_chunks\n",
    "    #         wav = np.array([wav[i * hop : i * hop + self.num_samples] for i in range(self.num_chunks)])\n",
    "    #     return wav\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.song_list[index]\n",
    "\n",
    "        # get genre\n",
    "        genre_name = line.split('/')[0]\n",
    "        genre_index = self.genres.index(genre_name)\n",
    "\n",
    "        # get audio\n",
    "        audio_filename = os.path.join(self.data_path, 'genres', line)\n",
    "        self.samples, self.sample_rate = lb.load(audio_filename)\n",
    "\n",
    "\n",
    "        # # adjust audio length\n",
    "        # wav = self._adjust_audio_length(wav).astype('float32')\n",
    "\n",
    "        # data augmentation\n",
    "        if self.is_augmentation:\n",
    "            samples = self.augmentation(torch.from_numpy(self.samples).unsqueeze(0)).squeeze(0).numpy()\n",
    "\n",
    "        return samples, genre_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_list)\n",
    "\n",
    "def get_dataloader(data_path=None, \n",
    "                   split='train', \n",
    "                   batch_size=16, \n",
    "                   num_workers=0, \n",
    "                   is_augmentation=False):\n",
    "    is_shuffle = True if (split == 'train') else False\n",
    "    data_loader = data.DataLoader(dataset=GTZANDataset(data_path, \n",
    "                                                       split, \n",
    "                                                       is_augmentation),\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=is_shuffle,\n",
    "                                  drop_last=False,\n",
    "                                  num_workers=num_workers)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GTZANDataset' object has no attribute 'samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\notebooks\\new\\CNN_modeling.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loader \u001b[39m=\u001b[39m get_dataloader(data_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../../datasets/genres_original\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, is_augmentation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m iter_train_loader \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(train_loader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_wav, train_genre \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iter_train_loader)\n",
      "\u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\notebooks\\new\\CNN_modeling.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_dataloader\u001b[39m(data_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m                    split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m                    batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m                    num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m                    is_augmentation\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     is_shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m (split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     data_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(dataset\u001b[39m=\u001b[39mGTZANDataset(data_path, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m                                                        split, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m                                                        is_augmentation),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m                                   batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m                                   shuffle\u001b[39m=\u001b[39mis_shuffle,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m                                   drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m                                   num_workers\u001b[39m=\u001b[39mnum_workers)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data_loader\n",
      "\u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\notebooks\\new\\CNN_modeling.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_song_list()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m is_augmentation:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_augmentations()\n",
      "\u001b[1;32mc:\\Users\\Nacho\\Documents\\MLOps\\Audiogmentation\\notebooks\\new\\CNN_modeling.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_get_augmentations\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     augment \u001b[39m=\u001b[39m Compose([\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         AddGaussianNoise(min_amplitude\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, max_amplitude\u001b[39m=\u001b[39m\u001b[39m0.015\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m         TimeStretch(min_rate\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, max_rate\u001b[39m=\u001b[39m\u001b[39m1.25\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         PitchShift(min_semitones\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m, max_semitones\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         Shift(p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     ])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nacho/Documents/MLOps/Audiogmentation/notebooks/new/CNN_modeling.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugmentation \u001b[39m=\u001b[39m augment(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_rate)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GTZANDataset' object has no attribute 'samples'"
     ]
    }
   ],
   "source": [
    "train_loader = get_dataloader(data_path='../../datasets/genres_original', split='train', is_augmentation=True)\n",
    "iter_train_loader = iter(train_loader)\n",
    "train_wav, train_genre = next(iter_train_loader)\n",
    "\n",
    "valid_loader = get_dataloader(split='valid')\n",
    "test_loader = get_dataloader(split='test')\n",
    "iter_test_loader = iter(test_loader)\n",
    "test_wav, test_genre = next(iter_test_loader)\n",
    "print('training data shape: %s' % str(train_wav.shape))\n",
    "print('validation/test data shape: %s' % str(test_wav.shape))\n",
    "print(train_genre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
